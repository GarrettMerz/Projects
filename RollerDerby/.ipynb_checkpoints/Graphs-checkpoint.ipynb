{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset, we can start to do some actual analysis. I'm going to be attempting to replicate the methodology of this paper:\n",
    "\n",
    "Sapienza, Anna and Goyal, Palash and Ferrara, Emilio. Deep Neural Networks for Optimal Team Composition. Frontiers in Big Data, vol 2. Jun 2019. https://arxiv.org/abs/1805.03285 \n",
    "\n",
    "While roller derby and esports games like League of Legends obviously are very different, in many ways, they can be treated similarly- each League match and individual jam of a derby bout consists of a team of 5 players with different defined roles attempting to achieve an objective while slowing the opposing team's attempt to achieve theirs.\n",
    "\n",
    "A derby bout (game) consists of a series of many individual jams. Each team forwards a defensive line of four \"blockers\" and an offensive line of one \"jammer\". The jammer scores points by passing through the \"pack\" of blockers- one initial non-scoring pass through the pack is required, and then one point is earned for each of the opposing team's blockers that the jammer passes on subsequent laps. Each jam can run for a set amount of time, but the jammer that is the first to complete the non-scoring pass (\"lead jammer\") can choose to end the jam early. In addition, the jammer can hand off their jammer status to one special blocker on each team called a \"pivot\" by passing the special helmet cover that the jammer wears. This is the general gist of the sport- in many ways, it's similar to the playground game \"Red Rover\", but on wheels.\n",
    "\n",
    "Naturally, when the blockers try to stop the jammer, things can get scrappy! Various penalties are given when a player shoves another in an illegal manner, when a blocker strays too far from the pack, when a player goes out of bounds, when a blocker makes an illegal formation (such as linking arms with another blocker), etc. It's general \"derby wisdom\" that certain penalties are more common \"new-skater\" penalties, while the distribution of penalties changes with skill. We can test this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's pick a team. I'll use the Kalamazoo Derby Darlins, the team I've announced for for the past few years. \n",
    "\n",
    "In this analysis, I'm going to make some assumptions.\n",
    "-First, that the fundamental unit of derby is not the bout, but the jam. Each jam is unique, and may have starting conditions determined by the preceding jam, but ultimately, for the purposes of this analysis, the only influence jam 1 may have on a jam like jam 20 is player stamina (N.B.: sometimes players can still be in the penalty box from previous jams, so this is not strictly correct! but it's probably correct enough for what we'd like to test here). This means that I will update a player's \"rating\" each jam rather than each bout.\n",
    "\n",
    "-Second, that the \"figure of merit\" to determine the performance of a jammer is the total number of points they score in a jam, but that the \"figure of merit\" to determine the performance of a blocker line is the difference between their jammer's score and the opposing jammer's score. A good blocker line is able to slow the opposing jammer substantially while also letting their own through.\n",
    "\n",
    "-Third: the rules of roller derby change often, as the sport is still relatively new. For instance- at one point, jammers scored an additional point for passing the opposing team's jammer as well as blockers. I'm assuming that we can largely treat them as constant- otherwise, I'm not sure we'll have enough stats.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trueskill\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import product\n",
    "from urllib.request import urlopen\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import to_agraph \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "import nbimporter\n",
    "import Webscraper as wsc\n",
    "\n",
    "\n",
    "teamID=str(3637)\n",
    "teamName='Killamazoo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstats(teamID,teamName):\n",
    "#First, get the lineups for each jam KDD has stats available for.\n",
    "    AllLineups = wsc.GetAllLineups(teamID, teamName)\n",
    "\n",
    "# Also, get expanding average of score differentials for each jam. We'll use a player's\n",
    "# average score differential after a given jam as a proxy for their skill ranking as measured\n",
    "# after playing that jam.\n",
    "\n",
    "    AllAvgs = wsc.ExpandingAverages(teamID, teamName)\n",
    "    badjams,badblockers = wsc.GetBadJamsAndBlockers(teamID, teamName,20)\n",
    "    \n",
    "    return AllLineups,AllAvgs,badjams,badblockers\n",
    "#print(badjams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only look at blockers for now, since they interact most closely with each other. Matching jammers to blocker lines is a different question than composing the lines themselves, since interplay is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(AllLineups, AllAvgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the short term play network described in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGraphs(teamID,teamName):\n",
    "    \n",
    "    AllLineups,AllAvgs,badjams,badblockers = getstats(teamID,teamName)\n",
    "    blockerlines = AllLineups[['B1', 'B2', 'B3', 'B4']]\n",
    "    #print(blockerlines)\n",
    "\n",
    "    STjams=[]\n",
    "    for jamnum in range(len((blockerlines.index))):\n",
    "\n",
    "        if (jamnum in badjams): continue\n",
    "        G = nx.complete_graph(4, nx.DiGraph())\n",
    "        blockers = blockerlines.iloc[jamnum].to_list()\n",
    "        mapping = dict(zip(G, blockers))\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        for edge in G.edges():\n",
    "            weight = AllAvgs.iloc[jamnum][edge[0]]-AllAvgs.iloc[jamnum-1][edge[0]]\n",
    "            #print(weight)\n",
    "            G[edge[0]][edge[1]]['weight'] = weight\n",
    "            STjams.append(G)\n",
    "\n",
    "    STGraph = nx.DiGraph()\n",
    "    for jam in STjams:\n",
    "        for edge in jam.edges():\n",
    "            if STGraph.has_edge(*edge):\n",
    "                weightsum = jam.get_edge_data(*edge)['weight'] + STGraph.get_edge_data(*edge)['weight'] \n",
    "                STGraph[edge[0]][edge[1]]['weight'] = weightsum\n",
    "            else: \n",
    "                #print(\"no edge yet\")\n",
    "                STGraph.add_edge(*edge[:2])\n",
    "                STGraph[edge[0]][edge[1]]['weight'] = 0\n",
    "\n",
    "    #Now get LTGraph.            \n",
    "    #Get nodes and edges from the STGraph, remove weights\n",
    "    LTGraph = STGraph.to_directed()\n",
    "\n",
    "    for edge in LTGraph.edges():\n",
    "        LTGraph[edge[0]][edge[1]]['weight'] = 0\n",
    "        LTGraph[edge[0]][edge[1]]['jamssince'] = 0\n",
    "        LTGraph[edge[0]][edge[1]]['totalcoplays'] = 0\n",
    "\n",
    "\n",
    "    #Add a new edge feature: \"jams since last co-play\" that updates each jam, and use it to get the weights    \n",
    "\n",
    "    for jamnum in range(len((blockerlines.index))):\n",
    "        #get all edges in jam\n",
    "        G = nx.complete_graph(4, nx.DiGraph())\n",
    "        blockers = blockerlines.iloc[jamnum].to_list()\n",
    "        mapping = dict(zip(G, blockers))\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        #get all possible combos\n",
    "        for edge in LTGraph.edges():\n",
    "            #zero if they play together in this jam, increment otherwise\n",
    "            if edge in G.edges(): LTGraph[edge[0]][edge[1]]['jamssince'] = 0\n",
    "            else: LTGraph[edge[0]][edge[1]]['jamssince'] += 1\n",
    "\n",
    "        #get total number of co-play jams    \n",
    "        for edge in G.edges():    \n",
    "            if edge in LTGraph.edges(): LTGraph[edge[0]][edge[1]]['totalcoplays'] += 1\n",
    "        \n",
    "        if (jamnum in badjams): continue\n",
    "        \n",
    "        # Get all blockers in the jam, get all possible teammates\n",
    "        for node in G:\n",
    "            edges = LTGraph.out_edges(node)\n",
    "            for edge in edges:\n",
    "            # weight them by exp(-time) since last co-play: influence persists across jams but drops off with time\n",
    "                nomweight = AllAvgs.iloc[jamnum][edge[0]]-AllAvgs.iloc[jamnum-1][edge[0]]\n",
    "                #print(LTGraph[edge[0]][edge[1]]['jamssince'])\n",
    "                modifier = np.exp(-LTGraph[edge[0]][edge[1]]['jamssince'])\n",
    "                LTGraph[edge[0]][edge[1]]['weight'] += nomweight*modifier\n",
    "    \n",
    "    return STGraph,LTGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PruneGraphs(STGraph,LTGraph):\n",
    "   \n",
    "    #print(len(STGraph))\n",
    "    edges_to_prune=[]\n",
    "    nodes_to_prune=[]\n",
    "    \n",
    "    #drop all edges with fewer than two co-plays    \n",
    "    for edge in LTGraph.edges():\n",
    "        thisedge = LTGraph.get_edge_data(*edge)\n",
    "        #print(thisedge)\n",
    "        if LTGraph[edge[0]][edge[1]]['totalcoplays'] < 2: \n",
    "            edges_to_prune.append(edge)\n",
    "    \n",
    "    for edge in edges_to_prune:\n",
    "        STGraph.remove_edge(*edge)\n",
    "        LTGraph.remove_edge(*edge)\n",
    "\n",
    "    #get Largest Connected Component\n",
    "    #if(nx.strongly_connected_components(STGraph) == []): \n",
    "    #    largestSTGraph = []\n",
    "    #    largestLTGraph = [] \n",
    "    \n",
    "   # else:\n",
    "    largestSTGraph = max(nx.strongly_connected_components(STGraph), key=len)\n",
    "    largestLTGraph = max(nx.strongly_connected_components(LTGraph), key=len)\n",
    "    \n",
    "    #print(STGraph)\n",
    "    for node in LTGraph: \n",
    "        if node not in largestLTGraph: nodes_to_prune.append(node)\n",
    "            #print(node)\n",
    "            \n",
    "    for node in nodes_to_prune:\n",
    "        #print(node)\n",
    "        STGraph.remove_node(node)\n",
    "        LTGraph.remove_node(node)\n",
    "\n",
    "    '''    \n",
    "    #Finally, normalize the graphs so edge weights sum to 1.0\n",
    "    LTtotweight = 0\n",
    "    STtotweight = 0\n",
    "    \n",
    "    for edge in LTGraph.edges():\n",
    "        LTtotweight += LTGraph[edge[0]][edge[1]]['weight'] \n",
    "        \n",
    "    for edge in STGraph.edges():    \n",
    "        STtotweight += STGraph[edge[0]][edge[1]]['weight'] \n",
    "        \n",
    "    for edge in LTGraph.edges():\n",
    "        LTGraph[edge[0]][edge[1]]['weight'] = LTGraph[edge[0]][edge[1]]['weight'] / LTtotweight  \n",
    "        \n",
    "    for edge in STGraph.edges():    \n",
    "        STGraph[edge[0]][edge[1]]['weight'] = STGraph[edge[0]][edge[1]]['weight'] / STtotweight  \n",
    "    \n",
    "        \n",
    "    #print(TGraph)\n",
    "    '''\n",
    "    return(STGraph,LTGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STGraph, LTGraph = GetGraphs(teamID,teamName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STpruned, LTpruned = PruneGraphs(STGraph, LTGraph)\n",
    "#print(nx.is_strongly_connected(STpruned))\n",
    "#nx.drawing.nx_pylab.draw_circular(STpruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAndWritePrunedGraphs(teamID,teamName):\n",
    "    STGraph, LTGraph = GetGraphs(teamID,teamName)\n",
    "    \n",
    "    try:\n",
    "        STpruned, LTpruned = PruneGraphs(STGraph, LTGraph)\n",
    "\n",
    "        #ST_relabel = nx.convert_node_labels_to_integers(STpruned)\n",
    "        #LT_relabel = nx.convert_node_labels_to_integers(LTpruned)\n",
    "\n",
    "        nx.write_weighted_edgelist(STpruned, \"Data/STGraphs/\"+teamID+\"STGraph.edgelist\", delimiter=\",,\")\n",
    "        nx.write_weighted_edgelist(LTpruned, \"Data/LTGraphs/\"+teamID+\"LTGraph.edgelist\", delimiter=\",,\")\n",
    "    \n",
    "    except: print(\"not enough data to get LCC!\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GetAndWritePrunedGraphs(str(3637),'Killamazoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Webscraper.ipynb:10: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 10 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  {\n",
      "Webscraper.ipynb:19: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 19 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"    from bs4 import BeautifulSoup\\n\",\n",
      "Webscraper.ipynb:7: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 7 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"In this series of notebooks, I will attempt to do some introductory exploration of various roller derby statistics. We will use the publicly available stats on the FlatTrackStats website. First, I will build a table scraper tool using the BeautifulSoup4 package to parse the stats tables on the website. If not already installed, you will need pandas and BeautifulSoup4 in order to run this notebook. \"\n",
      "Webscraper.ipynb:16: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"    import requests\\n\",\n"
     ]
    }
   ],
   "source": [
    "#Now make all STGraphs and LTGraphs\n",
    "\n",
    "IDs, names = wsc.getAllTeamsAndNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20988', '8003', '3636', '3422', '9248', '3420', '13529', '7876', '3433', '3399', '3424', '3418', '15073', '4737', '12607', '26170', '18437', '3404', '14228', '3464', '3397', '17350', '8143', '7521', '8059', '3402', '16730', '11127', '3419', '3437', '7928', '4740', '11444', '3414', '4742', '8142', '16731', '8141', '8087', '3463', '3642', '4744', '8140', '3395', '7870', '10187', '8052', '13834', '3427', '7511', '7813', '7696', '7608', '25344', '3432', '13840', '20989', '3400', '14613', '3444', '48273', '3625', '3431', '3471', '3644', '29611', '3392', '8044', '3435', '3430', '28777', '9085', '3406', '3457', '25351', '12621', '8138', '3465', '8731', '3396', '8095', '11351', '8086', '3466', '9086', '3411', '3640', '21447', '8137', '3626', '3426', '8047', '4036', '3413', '3412', '7825', '3643', '8073', '16733', '3456', '3447', '14233', '8127', '25640', '3421', '3646', '7244', '3627', '5916', '32345', '3647', '5917', '3467', '4292', '3637', '3407', '15020', '3470', '3639', '5918', '7745', '3416', '6738', '3425', '13612', '3443', '8046', '11255', '25759', '3453', '12502', '3428', '8542', '7593', '8610', '7815', '8105', '13528', '11525', '26479', '3438', '3445', '11403', '3629', '13401', '3429', '3454', '3423', '27351', '3393', '13100', '3450', '7247', '3446', '3452', '27179', '3468', '8002', '7761', '58599', '24211', '3394', '3401', '12490', '22848', '3415', '7614', '3641', '4291', '3403', '6740', '21262', '8103', '3459', '5919', '7822', '8628', '29706', '4741', '3405', '47144', '8132', '3409', '33110', '7845', '8124', '10178', '8118', '14194', '18716', '3638', '8894', '24511', '13403', '7848', '3633', '13732', '3449', '3469', '8131', '3635', '3648', '8102', '8101', '21403', '7248', '11470', '4738', '13397', '3417', '3458', '3462', '3440', '3408', '3624', '8057', '18376', '5920', '3398', '8048', '8661', '6742', '12233', '23027', '13957', '8518']\n",
      "[\" A'Salt Creek\", ' Acadiana', ' NEO', ' Alamo City', ' Albany', ' Angel City', ' Ann Arbor', ' Appalachian Rollergirls', ' Arch Rival', ' Arizona', ' Assassination', ' Atlanta', ' MRD: Unholy Rollers', ' Babe City', ' Diamond Divas', ' Bangor', ' Cereal Killers', ' Bay Area', ' Bay State Brawlers', ' Bellingham', ' Big Easy', ' Black Diamond', ' Black Rose Rollers', ' Black-n-Bluegrass', ' Blue Ridge Rollers', ' Boston', ' Boulder County', ' Brandywine', ' Brewcity', ' Burning River', ' Cajun Rollergirls', ' Cape Fear', ' Crushers', ' Carolina', ' Castle Rock', ' Mississippi Valley', ' Confluence', ' Cen-Tex', ' Shasta', ' Central Coast', ' Central NY', ' Charlotte', ' Charlottesville', ' Charm City', ' Chattanooga', ' Chemical Valley', ' Cherry City', ' DEAD: Superior Sirens', ' Cincinnati', ' Circle City', ' Classic City', ' Columbia', ' CoMo', ' Confluence', ' Connecticut', ' Cornfed', ' Crossroads City', ' Dallas', ' Dark River', ' DC', ' Dead River', ' Demolition', ' Denver', ' Derby City', ' Bakersfield', ' Team United', ' Detroit', ' Diamond State', ' Dixie', ' Dominion', ' LDV: Capital Corruption', ' Dub City', ' Duke', ' Dutchland', ' Paradise Roller Girls', ' Ohio Valley', ' Cen-Tex', ' Emerald City', ' Enchanted Mountain', ' Sin City', ' Fargo Moorhead', ' Rogue Rollergirls', ' Flint City', ' FoCo', ' Ft. Myers', ' Ft. Wayne', ' Fox City', ' Free State', ' Nashville', ' Garden State', ' Gem City', ' Glass City', ' Gold Coast', ' Gotham', ' Grand Raggidy', ' Granite State', ' Green Mt.', ' Greenville Derby Dames', ' Happy Valley', ' Hard Knox', ' Harrisburg', ' Black Rose Rollers', ' Hellions', ' First Settlement', ' Houston', ' Hudson Valley', ' Humboldt', ' ICT', ' Ithaca', ' WRD: Bonneville Bone Crushers', ' Jacksonville', ' Jersey Shore', ' Jet City', ' Junction City', ' Killamazoo', ' Kansas City', ' Lansing Derby', ' Humboldt', ' Lehigh Valley', ' Little City', ' Little Steel Derby Girls', ' Long Island', ' Lowcountry', ' Madison', ' Billings', ' Maine', ' Mason-Dixon', ' New Hampshire', ' Med City', ' Memphis', ' Mid-State', ' Minnesota', \" Brawlin' Betties\", ' Mississippi Valley', ' Mo-Kan', ' Molly Roger', ' Santa Cruz', ' Morgantown', ' Mother State', ' Muscogee', ' Naptown', ' Nashville', \" A'Salt Creek\", ' New Hampshire', ' Shore Points', ' No Coast', ' North Star', ' NW Arkansas', ' Northwest', ' Ohio', ' Ohio Valley', ' Okla. Victory', ' Old Capitol City', ' Oly', ' Omaha', ' Psycho City', ' Paradise Roller Girls', ' SK8R Dolls', ' Paradise Roller Girls', ' Peach State', ' Silicon Valley', ' Philly', ' Pikes Peak', ' Port Scandalous', ' Punishers', ' Providence', ' South Bend', ' Queen City', ' Oklahoma City', ' Rat City', ' Red Stick', ' Resurrection', ' Richland County Regulators', ' River City', ' Roc City', ' Rock Coast', ' Central Arkansas', \" Rockin' City\", ' Rocktown', ' Rocky Mtn.', ' Roe City Rollers', ' R.O.C.K.', ' Rose', ' Confluence', ' Sac City', ' St. Chux', ' Salisbury', ' Assault City', ' San Diego Starlettes', ' San Fernando', ' Santa Cruz', ' Savannah', ' Shasta', ' Shore Points', ' Sick Town', ' Silicon Valley', ' SINtral Valley', ' Sioux Falls', ' Slaughterhouse', ' SoCal Derby', ' Pueblo', ' Sonoma', ' Soul City', ' South Bend', ' Southern Delaware', ' So Ill', ' Spokannibals', ' Springfield', ' State College', ' Steel City', ' Suburbia', ' Tallahassee', ' Tampa', ' Texas', ' Chicago Outfit', ' Tragic City', ' Traverse City', ' Treasure Valley', ' Tucson', ' Twin City Derby Girls', ' Knockers', ' New River Valley', ' Undead Bettys', ' Unforgiven', ' V Town', ' Ventura']\n"
     ]
    }
   ],
   "source": [
    "print(IDs)\n",
    "print(names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20988  A'Salt Creek\n",
      "8003  Acadiana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Webscraper.ipynb:7: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 7 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"In this series of notebooks, I will attempt to do some introductory exploration of various roller derby statistics. We will use the publicly available stats on the FlatTrackStats website. First, I will build a table scraper tool using the BeautifulSoup4 package to parse the stats tables on the website. If not already installed, you will need pandas and BeautifulSoup4 in order to run this notebook. \"\n",
      "Webscraper.ipynb:16: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"    import requests\\n\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3636  NEO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Webscraper.ipynb:7: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 7 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"In this series of notebooks, I will attempt to do some introductory exploration of various roller derby statistics. We will use the publicly available stats on the FlatTrackStats website. First, I will build a table scraper tool using the BeautifulSoup4 package to parse the stats tables on the website. If not already installed, you will need pandas and BeautifulSoup4 in order to run this notebook. \"\n",
      "Webscraper.ipynb:16: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"    import requests\\n\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3422  Alamo City\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Webscraper.ipynb:7: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 7 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"In this series of notebooks, I will attempt to do some introductory exploration of various roller derby statistics. We will use the publicly available stats on the FlatTrackStats website. First, I will build a table scraper tool using the BeautifulSoup4 package to parse the stats tables on the website. If not already installed, you will need pandas and BeautifulSoup4 in order to run this notebook. \"\n",
      "Webscraper.ipynb:16: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 16 of the file Webscraper.ipynb. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  \"    import requests\\n\",\n"
     ]
    }
   ],
   "source": [
    "for ID,name in zip(IDs,names):\n",
    "    print(ID,name)\n",
    "    GetAndWritePrunedGraphs(str(ID),name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
