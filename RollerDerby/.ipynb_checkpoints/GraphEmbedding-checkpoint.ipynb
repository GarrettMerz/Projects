{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's make a model. The paper I'm modelling this project on uses a modified version of Structural Deep Neighbor Embedding (SDNE): https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf. There's a very nice implementation of SDNE in keras (along with some other embeddings) here: https://github.com/palash1992/GEM. Instead of creating SDNE from scratch, I'm just going to modify the SDNE model in the GEM package.\n",
    "\n",
    "The SDNE model in the GEM package takes one large graph and trains on batches of its edges. There are a few ways we could extend this to our problem- we could pad each team's graph with isolated nodes and feed each team's graph in one at a time, or we could create one big graph formed of several disjoint connected-component graphs consisting of each team's roster. The latter makes more sense here- we want to learn the embedding of each individual player node in the training set and how the properties of each node can be used to predict unobserved edges, not necessarily a set of graph properties that can be generalized across teams (which is a harder problem). In other words, for now, I'm interpolating, not extrapolating. For generalizable properties, we'd likely need to look at stuff like GCNs or GAT models- things that learn bigger structural properties.\n",
    "\n",
    "There is one small assumption that goes into this- that two blockers with the same name are the same player. Derby names are punny, so it's not rare that a good player name like 'Smackpropagation' will show up on a few different teams. However, the nature of our pruning algorithm tends to only give us data for graphs for decently established teams, and this name-similarity problem tends to only show up with more local teams (who are less likely to have enough stats on the website).\n",
    "\n",
    "The Teammate algorithm randomly removes 20% of the edges from the network, sets this aside as a test network and the non-removed 80% as a train network. We need to do this too.\n",
    "\n",
    "Because we are only learning observed edges, not null ones, and the random train-test subsampling procedure described in the Team Composition paper can create disconnected graphs that are then fed into the autoencoder, it doesn't seem like strongly-connectedness is necessarily important for the Teammate algorithm to work (just that it's considered good form). Spectral-decomposition based algorithms do require connectivity, but I don't believe this one does.\n",
    "\n",
    "The Teammate paper uses random-walk sampling on the test set. Because our graph is made of several disjoint components rather than one large LCC, I'm also not entirely convinced this step will do anything for us, so I'll leave it out for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Train on the train network. The loss function should be the same as SDNE i.e. (L = $\\sum_{i=1}^{n} |(\\hat{x}_{i}-x_{i})  \\odot [b_{(i,j)}]_{j=1}^{n}|^{2}$) but with the follwing change:\n",
    "\n",
    "Base SDNE: $b_{(i,j)} = 1$ if $(i,j)= 0, b_{(i,j)} = \\beta > 1$ otherwise.\n",
    "\n",
    "Teammate Encoder SDNE: $b_{(i,j)} = 0$ if $(i,j)= 0, b_{(i,j)} = 1$ otherwise.\n",
    "\n",
    "In standard SDNE, the loss function penalizes zero edges. However, we don't just want to penalize them- we want to completely ignore them. We're not trying to predict *if* players have played together, just how well they do play together when they do. The base SDNE also has a parameter $\\alpha$ that controls the first-order loss via the method of Laplacian Eigenmaps, but if we set this to zero we will reconstruct the 'simple' loss function given in the Teammate paper.\n",
    "\n",
    "These modified embedding methods are implemented in 'teammate.py' and 'teammate_utils.py',\n",
    "\n",
    "We'll also need to implement the MSE and MANE methods of measuring graph reconstruction ability. I've added these metrics to the 'metrics.py' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the graph embedding methods on Karate graph and evaluate them on \n",
    "graph reconstruction and visualization. Please copy the \n",
    "gem/data/karate.edgelist to the working directory\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "from gem.utils      import graph_util, plot_util\n",
    "from gem.evaluation import visualize_embedding as viz\n",
    "from gem.evaluation import evaluate_graph_reconstruction as gr\n",
    "\n",
    "from gem.embedding.gf       import GraphFactorization\n",
    "from gem.embedding.hope     import HOPE\n",
    "from gem.embedding.lap      import LaplacianEigenmaps\n",
    "from gem.embedding.lle      import LocallyLinearEmbedding\n",
    "from gem.embedding.node2vec import node2vec\n",
    "import networkx as nx\n",
    "from gem.embedding.teammate     import Teammate\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ''' Sample usage\n",
    "    python run_karate.py -node2vec 1\n",
    "    '''\n",
    "    parser = ArgumentParser(description='Graph Embedding Experiments on Roller Derby graphs')\n",
    "    parser.add_argument('-node2vec', '--node2vec',\n",
    "                        help='whether to run node2vec (default: False)')\n",
    "    args = vars(parser.parse_args())\n",
    "    try:\n",
    "        run_n2v = bool(int(args[\"node2vec\"]))\n",
    "    except:\n",
    "        run_n2v = False\n",
    "\n",
    "    # File that contains the edges. Format: source target\n",
    "    # Optionally, you can add weights as third column: source target weight\n",
    "    edge_tot = '../../Data/AllTeamsFullLTGraph.edgelist'\n",
    "    edge_train = '../../Data/AllTeamsLTGraphTrainStandardized.edgelist'\n",
    "    edge_test = '../../Data/AllTeamsLTGraphTestStandardized.edgelist'\n",
    "    edge_val = '../../Data/AllTeamsLTGraphValStandardized.edgelist'\n",
    "    # Specify whether the edges are directed\n",
    "    isDirected = True\n",
    "\n",
    "    # Load graph. Have to prune manually to keep number of nodes fixed\n",
    "    G = nx.read_weighted_edgelist(edge_tot, nodetype=int)\n",
    "    G_test_dummy = nx.read_weighted_edgelist(edge_test, nodetype=int)\n",
    "    G_train_dummy = nx.read_weighted_edgelist(edge_train, nodetype=int)\n",
    "    G_val_dummy = nx.read_weighted_edgelist(edge_val, nodetype=int)\n",
    "\n",
    "    G = G.to_directed()\n",
    "   G = G.to_directed()\n",
    "\n",
    "    G_train = G.copy()\n",
    "    G_val = G.copy()\n",
    "    G_test = G.copy()\n",
    "\n",
    "    for edge in G.edges():\n",
    "        if edge not in G_train_dummy.edges(): G_train.remove_edge(*edge)\n",
    "        if edge not in G_test_dummy.edges(): G_test.remove_edge(*edge)\n",
    "        if edge not in G_val_dummy.edges(): G_val.remove_edge(*edge)\n",
    "\n",
    "    print(len(G_train))\n",
    "    print(len(G_test))\n",
    "    print(len(G_val))\n",
    "\n",
    "    print(G.number_of_edges())\n",
    "    print(G_train.number_of_edges())\n",
    "    print(G_val.number_of_edges())\n",
    "    print(G_test.number_of_edges())\n",
    "    print(G_train_dummy.nodes)\n",
    "\n",
    "    models = []\n",
    "    # Load the models you want to run\n",
    "    # models.append(GraphFactorization(d=2, max_iter=50000, eta=1 * 10**-4, regu=1.0, data_set='derby'))\n",
    "    #models.append(HOPE(d=4, beta=0.01))\n",
    "    #models.append(LaplacianEigenmaps(d=2))\n",
    "    #models.append(LocallyLinearEmbedding(d=2))\n",
    "    if run_n2v:\n",
    "        models.append(\n",
    "            node2vec(d=2, max_iter=1, walk_len=80, num_walks=10, con_size=10, ret_p=1, inout_p=1)\n",
    "        )\n",
    "    #alpha = 0 to have \"traditional\" second order loss\n",
    "    models.append(Teammate(d=2, beta=5, alpha=0, nu1=1e-6, nu2=1e-6, K=3,n_units=[500, 300], rho=0.3, n_iter=50, xeta=0.001, n_batch=7,\n",
    "                    modelfile=['enc_model.json', 'dec_model.json'],\n",
    "                    weightfile=['enc_weights.hdf5', 'dec_weights.hdf5']))\n",
    "\n",
    "    # For each model, learn the embedding and evaluate on graph reconstruction and visualization\n",
    "    for embedding in models:\n",
    "        print ('Num nodes: %d, num edges: %d' % (G.number_of_nodes(), G.number_of_edges()))\n",
    "        t1 = time()\n",
    "        # Learn embedding - accepts a networkx graph or file with edge list\n",
    "        Y, t = embedding.learn_embedding(graph=G_train, edge_f=None, is_weighted=True, no_python=True)\n",
    "        print (embedding._method_name+':\\n\\tTraining time: %f' % (time() - t1))\n",
    "        print(Y)\n",
    " \n",
    "        # Evaluate on graph reconstruction:train\n",
    "        MAP, prec_curv, err, err_baseline = gr.evaluateStaticGraphReconstruction(G_train, embedding, Y, None, is_weighted=True, is_undirected=False)\n",
    "        print((\"\\tMAP: {} \\t precision curve: {}\\n\\n\\n\\n\"+'-'*100).format(MAP,prec_curv[:5]))\n",
    "        viz.plot_embedding2D(embedding.get_embedding(), di_graph=G_train, node_colors=None)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        # Evaluate on graph reconstruction:val\n",
    "        MAP, prec_curv, err, err_baseline = gr.evaluateStaticGraphReconstruction(G_val, embedding, Y, None, is_weighted=True, is_undirected=False)\n",
    "        print((\"\\tMAP: {} \\t precision curve: {}\\n\\n\\n\\n\"+'-'*100).format(MAP,prec_curv[:5]))\n",
    "        viz.plot_embedding2D(embedding.get_embedding(), di_graph=G_val, node_colors=None)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "\"\"\"\n",
    "        # Evaluate on graph reconstruction:val\n",
    "        MAP, prec_curv, err, err_baseline = gr.evaluateStaticGraphReconstruction(G_test, embedding, Y, None, is_weighted=True, is_undirected=False)\n",
    "        print((\"\\tMAP: {} \\t precision curve: {}\\n\\n\\n\\n\"+'-'*100).format(MAP,prec_curv[:5]))\n",
    "        viz.plot_embedding2D(embedding.get_embedding(), di_graph=G_test, node_colors=None)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
