{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset, we can start to do some actual analysis. I'm going to be attempting to replicate the methodology of this paper:\n",
    "\n",
    "Sapienza, Anna and Goyal, Palash and Ferrara, Emilio. Deep Neural Networks for Optimal Team Composition. Frontiers in Big Data, vol 2. Jun 2019. https://arxiv.org/abs/1805.03285 \n",
    "\n",
    "While roller derby and esports games like League of Legends obviously are very different, in many ways, they can be treated similarly- each League match and individual jam of a derby bout consists of a team of 5 players with different defined roles attempting to achieve an objective while slowing the opposing team's attempt to achieve theirs.\n",
    "\n",
    "A derby bout (game) consists of a series of many individual jams. Each team forwards a defensive line of four \"blockers\" and an offensive line of one \"jammer\". The jammer scores points by passing through the \"pack\" of blockers- one initial non-scoring pass through the pack is required, and then one point is earned for each of the opposing team's blockers that the jammer passes on subsequent laps. Each jam can run for a set amount of time, but the jammer that is the first to complete the non-scoring pass (\"lead jammer\") can choose to end the jam early. In addition, the jammer can hand off their jammer status to one special blocker on each team called a \"pivot\" by passing the special helmet cover that the jammer wears. This is the general gist of the sport- in many ways, it's similar to the playground game \"Red Rover\", but on wheels.\n",
    "\n",
    "Let's see if wee can build a teammate recommender system similar to the one in the paper. First, we'll need to make the play graphs for the autoencoder to encode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, I'm going to make some assumptions.\n",
    "-First, that the fundamental unit of derby is not the bout, but the jam. Each jam is unique, and may have starting conditions determined by the preceding jam, but ultimately, for the purposes of this analysis, the only influence jam 1 may have on a jam like jam 20 is player stamina (N.B.: sometimes players can still be in the penalty box from previous jams, so this is not strictly correct! but it's probably correct enough for what we'd like to test here). This means that I will update a player's \"rating\" each jam rather than each bout.\n",
    "\n",
    "-Second, that while the \"figure of merit\" to determine the performance of a jammer is the total number of points they score in a jam, but that the \"figure of merit\" to determine the performance of a blocker line is the difference between their jammer's score and the opposing jammer's score. A good blocker line is able to slow the opposing jammer substantially while also letting their own through.\n",
    "\n",
    "-Third: the rules of roller derby change often, as the sport is still relatively new. For instance- at one point, jammers scored an additional point for passing the opposing team's jammer as well as blockers. I'm assuming that we can largely treat them as constant- otherwise, I'm not sure we'll have enough stats to do meaningful analysis. This is perhaps the biggest assumption that goes into this project- the game is constantly evolving, and at high levels, play can change considerably as a result of rule changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trueskill\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import product\n",
    "from urllib.request import urlopen\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import to_agraph \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import random\n",
    "from random import sample\n",
    "\n",
    "import nbimporter\n",
    "import Webscraper as wsc\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstats(teamID,teamName):\n",
    "#First, get the lineups for each jam KDD has stats available for.\n",
    "    AllLineups = wsc.GetAllLineups(teamID, teamName)\n",
    "\n",
    "# Also, get expanding average of score differentials for each jam. We'll use a player's\n",
    "# average score differential after a given jam as a proxy for their skill ranking as measured\n",
    "# after playing that jam.\n",
    "\n",
    "    AllAvgs = wsc.ExpandingAverages(teamID, teamName)\n",
    "    badjams,badblockers = wsc.GetBadJamsAndBlockers(teamID, teamName,20)\n",
    "    \n",
    "    return AllLineups,AllAvgs,badjams,badblockers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only look at blockers for now, since they interact most closely with each other. Matching jammers to blocker lines is a different question than composing the lines themselves, since interplay is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the short and long term play networks described in the paper. We'll also need to prune them to remove isolated nodes and edges corresponding to less than two co-play jams. As an aside, there's a pretty clear typo in the paper: the long-term play network should drop off with time since last co-play, not increase (i.e., there should be a negative sign in the exponent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGraphs(teamID,teamName):\n",
    "    \n",
    "    AllLineups,AllAvgs,badjams,badblockers = getstats(teamID,teamName)\n",
    "    blockerlines = AllLineups[['B1', 'B2', 'B3', 'B4']]\n",
    "    #print(blockerlines)\n",
    "\n",
    "    STjams=[]\n",
    "    for jamnum in range(len((blockerlines.index))):\n",
    "\n",
    "        if (jamnum in badjams): continue\n",
    "        G = nx.complete_graph(4, nx.DiGraph())\n",
    "        blockers = blockerlines.iloc[jamnum].to_list()\n",
    "        mapping = dict(zip(G, blockers))\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        for edge in G.edges():\n",
    "            weight = AllAvgs.iloc[jamnum][edge[0]]-AllAvgs.iloc[jamnum-1][edge[0]]\n",
    "            #print(weight)\n",
    "            G[edge[0]][edge[1]]['weight'] = weight\n",
    "            STjams.append(G)\n",
    "\n",
    "    STGraph = nx.DiGraph()\n",
    "    for jam in STjams:\n",
    "        for edge in jam.edges():\n",
    "            if STGraph.has_edge(*edge):\n",
    "                weightsum = jam.get_edge_data(*edge)['weight'] + STGraph.get_edge_data(*edge)['weight'] \n",
    "                STGraph[edge[0]][edge[1]]['weight'] = weightsum\n",
    "            else: \n",
    "                #print(\"no edge yet\")\n",
    "                STGraph.add_edge(*edge[:2])\n",
    "                STGraph[edge[0]][edge[1]]['weight'] = 0\n",
    "\n",
    "    #Now get LTGraph.            \n",
    "    #Get nodes and edges from the STGraph, remove weights\n",
    "    LTGraph = STGraph.to_directed()\n",
    "\n",
    "    for edge in LTGraph.edges():\n",
    "        LTGraph[edge[0]][edge[1]]['weight'] = 0\n",
    "        LTGraph[edge[0]][edge[1]]['jamssince'] = 0\n",
    "        LTGraph[edge[0]][edge[1]]['totalcoplays'] = 0\n",
    "\n",
    "\n",
    "    #Add a new edge feature: \"jams since last co-play\" that updates each jam, and use it to get the weights    \n",
    "\n",
    "    for jamnum in range(len((blockerlines.index))):\n",
    "        #get all edges in jam\n",
    "        G = nx.complete_graph(4, nx.DiGraph())\n",
    "        blockers = blockerlines.iloc[jamnum].to_list()\n",
    "        mapping = dict(zip(G, blockers))\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        #get all possible combos\n",
    "        for edge in LTGraph.edges():\n",
    "            #zero if they play together in this jam, increment otherwise\n",
    "            if edge in G.edges(): LTGraph[edge[0]][edge[1]]['jamssince'] = 0\n",
    "            else: LTGraph[edge[0]][edge[1]]['jamssince'] += 1\n",
    "\n",
    "        #get total number of co-play jams    \n",
    "        for edge in G.edges():    \n",
    "            if edge in LTGraph.edges(): LTGraph[edge[0]][edge[1]]['totalcoplays'] += 1\n",
    "        \n",
    "        if (jamnum in badjams): continue\n",
    "        \n",
    "        # Get all blockers in the jam, get all possible teammates\n",
    "        for node in G:\n",
    "            edges = LTGraph.out_edges(node)\n",
    "            for edge in edges:\n",
    "            # weight them by exp(-time) since last co-play: influence persists across jams but drops off with time\n",
    "                nomweight = AllAvgs.iloc[jamnum][edge[0]]-AllAvgs.iloc[jamnum-1][edge[0]]\n",
    "                #print(LTGraph[edge[0]][edge[1]]['jamssince'])\n",
    "                modifier = np.exp(-LTGraph[edge[0]][edge[1]]['jamssince'])\n",
    "                LTGraph[edge[0]][edge[1]]['weight'] += nomweight*modifier\n",
    "    \n",
    "    return STGraph,LTGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PruneGraphs(STGraph,LTGraph):\n",
    "   \n",
    "    #print(len(STGraph))\n",
    "    edges_to_prune=[]\n",
    "    nodes_to_prune=[]\n",
    "    \n",
    "    #drop all edges with fewer than two co-plays    \n",
    "    for edge in LTGraph.edges():\n",
    "        thisedge = LTGraph.get_edge_data(*edge)\n",
    "        #print(thisedge)\n",
    "        if LTGraph[edge[0]][edge[1]]['totalcoplays'] < 2: \n",
    "            edges_to_prune.append(edge)\n",
    "    \n",
    "    for edge in edges_to_prune:\n",
    "        STGraph.remove_edge(*edge)\n",
    "        LTGraph.remove_edge(*edge)\n",
    "\n",
    "   # else:\n",
    "    largestSTGraph = max(nx.strongly_connected_components(STGraph), key=len)\n",
    "    largestLTGraph = max(nx.strongly_connected_components(LTGraph), key=len)\n",
    "    \n",
    "    #print(STGraph)\n",
    "    for node in LTGraph: \n",
    "        if node not in largestLTGraph: nodes_to_prune.append(node)\n",
    "            #print(node)\n",
    "            \n",
    "    for node in nodes_to_prune:\n",
    "        #print(node)\n",
    "        STGraph.remove_node(node)\n",
    "        LTGraph.remove_node(node)\n",
    "\n",
    "    return(STGraph,LTGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAndWritePrunedGraphs(teamID,teamName):\n",
    "    STGraph, LTGraph = GetGraphs(teamID,teamName)\n",
    "    \n",
    "    try:\n",
    "        STpruned, LTpruned = PruneGraphs(STGraph, LTGraph)\n",
    "\n",
    "        ST_relabel = nx.convert_node_labels_to_integers(STpruned)\n",
    "        LT_relabel = nx.convert_node_labels_to_integers(LTpruned)\n",
    "\n",
    "        nx.write_weighted_edgelist(STpruned, \"Data/STGraphs/\"+teamID+\"STGraph.edgelist\", delimiter=\",,\")\n",
    "        nx.write_weighted_edgelist(LTpruned, \"Data/LTGraphs/\"+teamID+\"LTGraph.edgelist\", delimiter=\",,\")\n",
    "    \n",
    "    except: print(\"not enough data to get LCC!\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure this method works for one team\n",
    "GetAndWritePrunedGraphs(str(3637),'Killamazoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now make all STGraphs and LTGraphs\n",
    "\n",
    "IDs, names = wsc.getAllTeamsAndNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IDs)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID,name in zip(IDs,names):\n",
    "    if path.exists(\"Data/STGraphs/\"+ID+\"STGraph.edgelist\"): continue\n",
    "    print(ID,name)\n",
    "    GetAndWritePrunedGraphs(str(ID),name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge these edgelists manually using shell commands (i.e., not as a part of this notebook) to make graphs called 'AllTeamsFullSTGraph.edgelist' and 'AllTeamsFullLTGraph.edgelist'. After getting these, let's relabel the nodes from names to indices, so NetworkX can handle them more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STGraphNames = nx.read_weighted_edgelist(\"Data/AllTeamsFullSTGraph.edgelist\", delimiter=\",,\")\n",
    "LTGraphNames = nx.read_weighted_edgelist(\"Data/AllTeamsFullLTGraph.edgelist\", delimiter=\",,\")\n",
    "STGraph = nx.convert_node_labels_to_integers(STGraphNames)\n",
    "LTGraph = nx.convert_node_labels_to_integers(LTGraphNames)\n",
    "\n",
    "nx.write_weighted_edgelist(STGraph, \"Data/AllTeamsFullSTGraph.edgelist\")\n",
    "nx.write_weighted_edgelist(LTGraph, \"Data/AllTeamsFullLTGraph.edgelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly split the full graph into train, val, and test edge sets. Note that these won't necessarily be connected- the val and test graphs may contain orphan nodes. However, this is ok for what we're doing: we don't want the autoencoder to only consider observed links, so unobserved links don't have any negative effect on the recommender's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeTrainValTest():\n",
    "    \n",
    "    STtrainset = []\n",
    "    STtestset = []\n",
    "    STvalset = []\n",
    "    \n",
    "    STGraphFullNorm = nx.read_weighted_edgelist(\"Data/AllTeamsFullSTGraph.edgelist\")\n",
    "    LTGraphFullNorm = nx.read_weighted_edgelist(\"Data/AllTeamsFullLTGraph.edgelist\")\n",
    "\n",
    "    print(len(LTGraphFullNorm.edges()))\n",
    "    #25126 total, so 20100 train, 2513 val, 2513 test\n",
    "    \n",
    "    testlistLT = random.sample(LTGraphFullNorm.edges(),2513)\n",
    "    trainvalLT = [x for x in LTGraphFullNorm.edges() if x not in testlistLT]\n",
    "    vallistLT = random.sample(trainvalLT,2513)\n",
    "    trainlistLT = [x for x in trainvalLT if x not in vallistLT]\n",
    "    \n",
    "    traingraphLT = LTGraphFullNorm.edge_subgraph(trainlistLT)\n",
    "    valgraphLT = LTGraphFullNorm.edge_subgraph(vallistLT)\n",
    "    testgraphLT = LTGraphFullNorm.edge_subgraph(testlistLT)    \n",
    "    nx.write_weighted_edgelist(traingraphLT, \"Data/AllTeamsLTGraphTrain.edgelist\")\n",
    "    nx.write_weighted_edgelist(valgraphLT, \"Data/AllTeamsLTGraphVal.edgelist\")\n",
    "    nx.write_weighted_edgelist(testgraphLT, \"Data/AllTeamsLTGraphTest.edgelist\")\n",
    "    \n",
    "    testlistST = random.sample(STGraphFullNorm.edges(),2513)\n",
    "    trainvalST = [x for x in STGraphFullNorm.edges() if x not in testlistST]\n",
    "    vallistST = random.sample(trainvalST,2513)\n",
    "    trainlistST = [x for x in trainvalST if x not in vallistST]\n",
    "    print(len(trainlistST),len(vallistST),len(testlistST))\n",
    "    traingraphST = STGraphFullNorm.edge_subgraph(trainlistST)\n",
    "    valgraphST = STGraphFullNorm.edge_subgraph(vallistST)\n",
    "    testgraphST = STGraphFullNorm.edge_subgraph(testlistST)    \n",
    "    nx.write_weighted_edgelist(traingraphST, \"Data/AllTeamsSTGraphTrain.edgelist\")\n",
    "    nx.write_weighted_edgelist(valgraphST, \"Data/AllTeamsSTGraphVal.edgelist\")\n",
    "    nx.write_weighted_edgelist(testgraphST, \"Data/AllTeamsSTGraphTest.edgelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some methods for preprocessing the data. Ultimately, min-max scaling seems to perform the best. In both short and long term networks, the distribution of weights is roughly Gaussian, so the data can be safely scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainStats():\n",
    "    STGraphTrainUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsSTGraphTrain.edgelist\")\n",
    "    LTGraphTrainUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsLTGraphTrain.edgelist\")\n",
    "    \n",
    "    STweights = []\n",
    "    LTweights = []\n",
    "    STweightsNew = []\n",
    "    LTweightsNew = [] \n",
    "\n",
    "    for node1, node2, data in STGraphTrainUnNorm.edges(data=True):\n",
    "        STweights.append(data['weight'])\n",
    "    \n",
    "    for node1, node2, data in LTGraphTrainUnNorm.edges(data=True):\n",
    "        LTweights.append(data['weight'])\n",
    "        \n",
    "    STsig = statistics.stdev(STweights)\n",
    "    STmean = statistics.mean(STweights)\n",
    "    STmin = min(STweights)\n",
    "    STmax = max(STweights)\n",
    "    LTsig = statistics.stdev(LTweights)\n",
    "    LTmean = statistics.mean(LTweights)\n",
    "    LTmin = min(LTweights)\n",
    "    LTmax = max(LTweights)    \n",
    "    \n",
    "    return STmean,STsig,STmin,STmax,LTmean,LTsig,LTmin,LTmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardizeGraph(graph,trainmean,trainstdev):\n",
    "    \n",
    "    weights = []\n",
    "    weightsNew = []\n",
    "\n",
    "    for node1, node2, data in graph.edges(data=True):\n",
    "        weights.append(data['weight'])\n",
    "    \n",
    "    #Both ST and LT graph weight dists are Gaussian, so we can safely standardize inputs without \n",
    "    #dramatically altering the structure of the data.\n",
    "\n",
    "    \n",
    "    for node1, node2, data in graph.edges(data=True):\n",
    "        data['weight'] = (data['weight'] - trainmean)/trainstdev\n",
    "        weightsNew.append(data['weight'])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeGraph(graph,trainmin,trainmax):\n",
    "    \n",
    "    weights = []\n",
    "    weightsNew = []\n",
    "\n",
    "    for node1, node2, data in graph.edges(data=True):\n",
    "        weights.append(data['weight'])\n",
    "    \n",
    "    #Normalize so weights are between zero and one\n",
    "    \n",
    "    for node1, node2, data in graph.edges(data=True):\n",
    "        data['weight'] = (data['weight'] - trainmin)/(trainmax-trainmin)\n",
    "        weightsNew.append(data['weight'])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakeTrainValTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STmean,STsig,STmin,STmax,LTmean,LTsig,LTmin,LTmax = GetTrainStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STGraphTrainUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsSTGraphTrain.edgelist\")\n",
    "LTGraphTrainUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsLTGraphTrain.edgelist\")\n",
    "STGraphTestUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsSTGraphTest.edgelist\")\n",
    "LTGraphTestUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsLTGraphTest.edgelist\")\n",
    "STGraphValUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsSTGraphVal.edgelist\")\n",
    "LTGraphValUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsLTGraphVal.edgelist\")\n",
    "\n",
    "FullSTGraphUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsFullSTGraph.edgelist\")\n",
    "FullLTGraphUnNorm = nx.read_weighted_edgelist(\"Data/AllTeamsFullLTGraph.edgelist\")\n",
    "\n",
    "\n",
    "#normed but not Standardized\n",
    "STGraphTrainScaled = NormalizeGraph(STGraphTrainUnNorm, STmin, STmax)\n",
    "LTGraphTrainScaled = NormalizeGraph(LTGraphTrainUnNorm, LTmin, LTmax)\n",
    "STGraphValScaled = NormalizeGraph(STGraphValUnNorm, STmin, STmax)\n",
    "LTGraphValScaled = NormalizeGraph(LTGraphValUnNorm, LTmin, LTmax)\n",
    "STGraphTestScaled = NormalizeGraph(STGraphTestUnNorm, STmin, STmax)\n",
    "LTGraphTestScaled = NormalizeGraph(LTGraphTestUnNorm, LTmin, LTmax)\n",
    "\n",
    "\n",
    "FullSTGraphScaled = NormalizeGraph(FullSTGraphUnNorm, STmin, STmax)\n",
    "FullLTGraphScaled = NormalizeGraph(FullLTGraphUnNorm, LTmin, LTmax)\n",
    "\n",
    "nx.write_weighted_edgelist(STGraphTrainScaled, \"Data/AllTeamsSTGraphTrainNormalized.edgelist\")\n",
    "nx.write_weighted_edgelist(LTGraphTrainScaled, \"Data/AllTeamsLTGraphTrainNormalized.edgelist\")\n",
    "nx.write_weighted_edgelist(STGraphValScaled, \"Data/AllTeamsSTGraphValNormalized.edgelist\")\n",
    "nx.write_weighted_edgelist(LTGraphValScaled, \"Data/AllTeamsLTGraphValNormalized.edgelist\")\n",
    "nx.write_weighted_edgelist(STGraphTestScaled, \"Data/AllTeamsSTGraphTestNormalized.edgelist\")\n",
    "nx.write_weighted_edgelist(LTGraphTestScaled, \"Data/AllTeamsLTGraphTestNormalized.edgelist\")\n",
    "\n",
    "nx.write_weighted_edgelist(FullSTGraphScaled, \"Data/AllTeamsFullSTGraphNormalized.edgelist\")\n",
    "nx.write_weighted_edgelist(FullLTGraphScaled, \"Data/AllTeamsFullLTGraphNormalized.edgelist\")\n",
    "\n",
    "#Standardized but not normed\n",
    "STGraphTrainNorm = StandardizeGraph(STGraphTrainUnNorm, STmean, STsig)\n",
    "LTGraphTrainNorm = StandardizeGraph(LTGraphTrainUnNorm, LTmean, LTsig)\n",
    "STGraphValNorm = StandardizeGraph(STGraphValUnNorm, STmean, STsig)\n",
    "LTGraphValNorm = StandardizeGraph(LTGraphValUnNorm, LTmean, LTsig)\n",
    "STGraphTestNorm = StandardizeGraph(STGraphTestUnNorm, STmean, STsig)\n",
    "LTGraphTestNorm = StandardizeGraph(LTGraphTestUnNorm, LTmean, LTsig)\n",
    "\n",
    "FullSTGraphNorm = StandardizeGraph(FullSTGraphUnNorm, STmean, STsig)\n",
    "FullLTGraphNorm = StandardizeGraph(FullLTGraphUnNorm, LTmean, LTsig)\n",
    "\n",
    "nx.write_weighted_edgelist(STGraphTrainNorm, \"Data/AllTeamsSTGraphTrainStandardized.edgelist\")\n",
    "nx.write_weighted_edgelist(LTGraphTrainNorm, \"Data/AllTeamsLTGraphTrainStandardized.edgelist\")\n",
    "nx.write_weighted_edgelist(STGraphValNorm, \"Data/AllTeamsSTGraphValStandardized.edgelist\")\n",
    "nx.write_weighted_edgelist(LTGraphValNorm, \"Data/AllTeamsLTGraphValStandardized.edgelist\")\n",
    "nx.write_weighted_edgelist(STGraphTestNorm, \"Data/AllTeamsSTGraphTestStandardized.edgelist\")\n",
    "nx.write_weighted_edgelist(LTGraphTestNorm, \"Data/AllTeamsLTGraphTestStandardized.edgelist\")\n",
    "\n",
    "nx.write_weighted_edgelist(FullSTGraphNorm, \"Data/AllTeamsFullSTGraphStandardized.edgelist\")\n",
    "nx.write_weighted_edgelist(FullLTGraphNorm, \"Data/AllTeamsFullLTGraphStandardized.edgelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks good! These graphs are ready to be fed into a machine learning algorithm. On to the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
